{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0b0a19",
   "metadata": {},
   "source": [
    "分段式處理 TF JP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bffc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902be4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>['INFP', 'INFP', 'INFJ', 'ENFP', 'ISTP', 'INTP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>['INTJ', 'INTP', 'ENFP', 'INTJ', 'INTP', 'INTP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>['INTJ', 'INFP', 'INFP', 'INTP', 'INTP', 'INTJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>['INTJ', 'ISFJ', 'INFP', 'INTP', 'INTP', 'INTP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>['ENTJ', 'INTP', 'ENFP', 'INTP', 'ENTJ', 'INTJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  ['INFP', 'INFP', 'INFJ', 'ENFP', 'ISTP', 'INTP...\n",
       "1  ENTP  ['INTJ', 'INTP', 'ENFP', 'INTJ', 'INTP', 'INTP...\n",
       "2  INTP  ['INTJ', 'INFP', 'INFP', 'INTP', 'INTP', 'INTJ...\n",
       "3  INTJ  ['INTJ', 'ISFJ', 'INFP', 'INTP', 'INTP', 'INTP...\n",
       "4  ENTJ  ['ENTJ', 'INTP', 'ENFP', 'INTP', 'ENTJ', 'INTJ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/JenMing/Desktop/MBTI/LSTM/mbti_to_LSTM_DF.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9723a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編碼轉換\n",
    "personality_mapping_output = {'TJ': 0,\n",
    "                        'TP': 1,\n",
    "                        'FJ': 2,\n",
    "                        'FP': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1c1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編碼轉換\n",
    "personality_mapping = {'INFJ': 0,\n",
    "                        'ENTP': 1,\n",
    "                        'INTP': 2,\n",
    "                        'INTJ': 3,\n",
    "                        'ENTJ': 4,\n",
    "                        'ENFJ': 5,\n",
    "                        'INFP': 6,\n",
    "                        'ENFP': 7,\n",
    "                        'ISFP': 8,\n",
    "                        'ISTP': 9,\n",
    "                        'ISFJ': 10,\n",
    "                        'ISTJ': 11,\n",
    "                        'ESTP': 12,\n",
    "                        'ESFP': 13,\n",
    "                        'ESTJ': 14,\n",
    "                        'ESFJ': 15 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c3b1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTJ_counts = dimension_counts['TJ']\\nTP_counts = dimension_counts['TP']\\nFJ_counts = dimension_counts['FJ']\\nFP_counts = dimension_counts['FP']\\nprint(f'T.J: '+ str(TJ_counts/total*100) +'%\\n')\\nprint(f'T.P: '+ str(TP_counts/total*100) +'%\\n')\\nprint(f'F.J: '+ str(FJ_counts/total*100) +'%\\n')\\nprint(f'F.P: '+ str(FP_counts/total*100) +'%\\n')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = []\n",
    "\n",
    "chars_to_remove = \"][' \"  \n",
    "total = 0\n",
    "dimension_counts = {'TJ': 0,\n",
    "                        'TP': 0,\n",
    "                        'FJ': 0,\n",
    "                        'FP': 0}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    dialogues = row[\"posts\"] #字串\n",
    "    target_personality = row[\"type\"]\n",
    "    \n",
    "    target_personality = target_personality[2] + target_personality[3]\n",
    "    for char in chars_to_remove:\n",
    "        dialogues = dialogues.replace(char, \"\")\n",
    "    \n",
    "    dialogues_list = dialogues.split(',')\n",
    "    \n",
    "    '''\n",
    "    for i in range(len(dialogues_list)):\n",
    "        dialogues_list[i] = dialogues_list[i][2] + dialogues_list[i][3]\n",
    "        if dialogues_list[i] == 'TJ':\n",
    "            dimension_counts['TJ'] += 1\n",
    "        elif dialogues_list[i] == 'TP':\n",
    "            dimension_counts['TP'] += 1\n",
    "        elif dialogues_list[i] == 'FJ':\n",
    "            dimension_counts['FJ'] += 1\n",
    "        elif dialogues_list[i] == 'FP':\n",
    "            dimension_counts['FP'] += 1\n",
    "        total += 1\n",
    "\n",
    "    '''\n",
    "    dialogue_ids = [personality_mapping[personality] for personality in dialogues_list]\n",
    "    target_personality_id = personality_mapping_output[target_personality]\n",
    "    \n",
    "    encoded_data.append((dialogue_ids, target_personality_id))\n",
    "'''\n",
    "TJ_counts = dimension_counts['TJ']\n",
    "TP_counts = dimension_counts['TP']\n",
    "FJ_counts = dimension_counts['FJ']\n",
    "FP_counts = dimension_counts['FP']\n",
    "print(f'T.J: '+ str(TJ_counts/total*100) +'%\\n')\n",
    "print(f'T.P: '+ str(TP_counts/total*100) +'%\\n')\n",
    "print(f'F.J: '+ str(FJ_counts/total*100) +'%\\n')\n",
    "print(f'F.P: '+ str(FP_counts/total*100) +'%\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c3b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動態計算 input_size\n",
    "max_dialogue_length = max(len(dialogue) for dialogue, _ in encoded_data)\n",
    "input_size = max_dialogue_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508fa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列並轉換為張量\n",
    "padded_dialogues = [torch.tensor(dialogue, dtype=torch.float32) for dialogue, _ in encoded_data]\n",
    "padded_dialogues = pad_sequence(padded_dialogues, batch_first=True)\n",
    "\n",
    "target_personality = torch.tensor([target for _, target in encoded_data], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4b7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 ANN 模型\n",
    "class PersonalityPredictionANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob=0.5):\n",
    "        super(PersonalityPredictionANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6046090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集切分為訓練集和驗證集\n",
    "train_dialogues, val_dialogues, train_target, val_target = train_test_split(padded_dialogues, target_personality, test_size=0.15, random_state=42)\n",
    "\n",
    "# 初始化模型\n",
    "hidden_size = 64\n",
    "output_size = len(personality_mapping_output)\n",
    "model = PersonalityPredictionANN(input_size, hidden_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f22d8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義損失函數和優化器 (CEL:分類問題 MSE:回归问题)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.MSELoss() \n",
    "weight_decay = 0.01\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5054bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#早停\n",
    "patience = 10  # 設定早期停止的耐心值\n",
    "best_val_loss = float('inf')\n",
    "counter = 0  # 用於計算連續的驗證損失沒有改善的次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b3d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Loss: 1.3806\n",
      "Validation Loss: 1.3365, Validation Accuracy: 35.8679%\n",
      "Epoch [2/60], Loss: 1.3517\n",
      "Validation Loss: 1.3264, Validation Accuracy: 37.4808%\n",
      "Epoch [3/60], Loss: 1.3386\n",
      "Validation Loss: 1.3172, Validation Accuracy: 39.4777%\n",
      "Epoch [4/60], Loss: 1.3303\n",
      "Validation Loss: 1.3148, Validation Accuracy: 39.7081%\n",
      "Epoch [5/60], Loss: 1.3181\n",
      "Validation Loss: 1.3079, Validation Accuracy: 39.3241%\n",
      "Epoch [6/60], Loss: 1.3089\n",
      "Validation Loss: 1.3014, Validation Accuracy: 39.4777%\n",
      "Epoch [7/60], Loss: 1.2977\n",
      "Validation Loss: 1.2988, Validation Accuracy: 39.5545%\n",
      "Epoch [8/60], Loss: 1.2940\n",
      "Validation Loss: 1.2953, Validation Accuracy: 38.7097%\n",
      "Epoch [9/60], Loss: 1.2887\n",
      "Validation Loss: 1.2954, Validation Accuracy: 40.0922%\n",
      "Epoch [10/60], Loss: 1.2812\n",
      "Validation Loss: 1.2934, Validation Accuracy: 39.7081%\n",
      "Epoch [11/60], Loss: 1.2799\n",
      "Validation Loss: 1.2891, Validation Accuracy: 39.8618%\n",
      "Epoch [12/60], Loss: 1.2711\n",
      "Validation Loss: 1.2883, Validation Accuracy: 40.2458%\n",
      "Epoch [13/60], Loss: 1.2699\n",
      "Validation Loss: 1.2856, Validation Accuracy: 40.3226%\n",
      "Epoch [14/60], Loss: 1.2624\n",
      "Validation Loss: 1.2840, Validation Accuracy: 39.7081%\n",
      "Epoch [15/60], Loss: 1.2583\n",
      "Validation Loss: 1.2851, Validation Accuracy: 40.0154%\n",
      "Epoch [16/60], Loss: 1.2542\n",
      "Validation Loss: 1.2844, Validation Accuracy: 39.9386%\n",
      "Epoch [17/60], Loss: 1.2512\n",
      "Validation Loss: 1.2852, Validation Accuracy: 39.7081%\n",
      "Epoch [18/60], Loss: 1.2474\n",
      "Validation Loss: 1.2833, Validation Accuracy: 39.8618%\n",
      "Epoch [19/60], Loss: 1.2424\n",
      "Validation Loss: 1.2819, Validation Accuracy: 40.0154%\n",
      "Epoch [20/60], Loss: 1.2410\n",
      "Validation Loss: 1.2828, Validation Accuracy: 40.1690%\n",
      "Epoch [21/60], Loss: 1.2389\n",
      "Validation Loss: 1.2841, Validation Accuracy: 40.4762%\n",
      "Epoch [22/60], Loss: 1.2368\n",
      "Validation Loss: 1.2867, Validation Accuracy: 39.3241%\n",
      "Epoch [23/60], Loss: 1.2333\n",
      "Validation Loss: 1.2847, Validation Accuracy: 39.6313%\n",
      "Epoch [24/60], Loss: 1.2345\n",
      "Validation Loss: 1.2832, Validation Accuracy: 40.3226%\n",
      "Epoch [25/60], Loss: 1.2297\n",
      "Validation Loss: 1.2841, Validation Accuracy: 40.1690%\n",
      "Epoch [26/60], Loss: 1.2243\n",
      "Validation Loss: 1.2879, Validation Accuracy: 39.4777%\n",
      "Epoch [27/60], Loss: 1.2246\n",
      "Validation Loss: 1.2830, Validation Accuracy: 40.3226%\n",
      "Epoch [28/60], Loss: 1.2214\n",
      "Validation Loss: 1.2866, Validation Accuracy: 39.8618%\n",
      "Epoch [29/60], Loss: 1.2159\n",
      "Validation Loss: 1.2907, Validation Accuracy: 39.9386%\n",
      "Early Stopping: Validation loss has not improved for 10 epochs. Stopping training.\n",
      "T.F: 22557/37758 \n",
      "Accuracy: 0.5974098204354045\n",
      "\n",
      "J.P: 23510/37758 \n",
      "Accuracy: 0.6226495047407172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/JenMing/Desktop/MBTI/ANN/Model/TFJP/note.txt\", \"w\") as f:\n",
    "    num_epochs = 60\n",
    "    dimension_counts = {'T/F': 0,\n",
    "                        'J/P': 0}\n",
    "    item_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 將模型設置為訓練模式\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for dialogue_batch, target_batch in zip(train_dialogues, train_target):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(dialogue_batch)\n",
    "            loss = criterion(outputs, target_batch.long())  # 使用交叉熵損失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_dialogues)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "        f.write(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\\n\")\n",
    "        \n",
    "        # 驗證模型\n",
    "        model.eval()  # 將模型設置為評估模式\n",
    "        correct_predictions = 0\n",
    "        total_samples = len(val_dialogues)\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for dialogue_batch, target_batch in zip(val_dialogues, val_target):\n",
    "                outputs = model(dialogue_batch)\n",
    "                loss = criterion(outputs, target_batch.long())  # 使用交叉熵損失\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_class = torch.argmax(outputs).item()\n",
    "                true_class = target_batch.item()\n",
    "\n",
    "                for personality, value in personality_mapping_output.items():\n",
    "                    if value == predicted_class:\n",
    "                        mbti_labels_pre = personality\n",
    "                        break\n",
    "                for personality, value in personality_mapping_output.items():\n",
    "                    if value == int(true_class):\n",
    "                        mbti_labels_tru = personality \n",
    "                        break\n",
    "\n",
    "                for n in range(4):\n",
    "                    if n == 0:\n",
    "                        if mbti_labels_pre[n] == mbti_labels_tru[n]:\n",
    "                            dimension_counts['T/F'] += 1\n",
    "                    elif n == 1:\n",
    "                        if mbti_labels_pre[n] == mbti_labels_tru[n]:\n",
    "                            dimension_counts['J/P'] += 1\n",
    "\n",
    "                if predicted_class == true_class:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "        item_count += total_samples\n",
    "        average_val_loss = val_loss / len(val_dialogues)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f\"Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {accuracy*100:.4f}%\")\n",
    "        f.write(f\"Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {accuracy*100:.4f}%\\n\")\n",
    "        \n",
    "        # 檢查驗證損失是否改善\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        # 如果連續一定次數（耐心值）驗證損失沒有改善，則停止訓練\n",
    "        if counter >= patience:\n",
    "            print(f\"Early Stopping: Validation loss has not improved for {patience} epochs. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    TF_counts = dimension_counts['T/F']\n",
    "    JP_counts = dimension_counts['J/P']\n",
    "   \n",
    "    print(f'T.F: {TF_counts}/{item_count} ')\n",
    "    print('Accuracy: '+ str(TF_counts/item_count)+'\\n')\n",
    "    print(f'J.P: {JP_counts}/{item_count} ')\n",
    "    print('Accuracy: '+ str(JP_counts/item_count)+'\\n')\n",
    "    \n",
    "    f.write(f'T.F: {TF_counts}/{item_count} ')\n",
    "    f.write('Accuracy: '+ str(TF_counts/item_count)+'\\n')\n",
    "    f.write(f'J.P: {JP_counts}/{item_count} ')\n",
    "    f.write('Accuracy: '+ str(JP_counts/item_count)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccbf92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"C:/Users/JenMing/Desktop/MBTI/ANN/Model/TFJP/best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d13fad",
   "metadata": {},
   "source": [
    "現在是先把training data的資料改成4個字元的 原因是如果要跟原本的模型在validation的地方和用的話 那資料必須要是一樣的 \n",
    "只是這樣改的話 TF跟JP就跟之前差不多了\n",
    "可能要嘗試用2字元 然後再跟之前的模型合用會比較好 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
