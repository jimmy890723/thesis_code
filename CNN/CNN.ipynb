{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffe871bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef784dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/JenMing/Desktop/MBTI/LSTM/mbti_to_LSTM_DF.csv')\n",
    "df.head()\n",
    "\n",
    "# 編碼轉換\n",
    "personality_mapping = {'INFJ': 0,\n",
    "                        'ENTP': 1,\n",
    "                        'INTP': 2,\n",
    "                        'INTJ': 3,\n",
    "                        'ENTJ': 4,\n",
    "                        'ENFJ': 5,\n",
    "                        'INFP': 6,\n",
    "                        'ENFP': 7,\n",
    "                        'ISFP': 8,\n",
    "                        'ISTP': 9,\n",
    "                        'ISFJ': 10,\n",
    "                        'ISTJ': 11,\n",
    "                        'ESTP': 12,\n",
    "                        'ESFP': 13,\n",
    "                        'ESTJ': 14,\n",
    "                        'ESFJ': 15 }\n",
    "\n",
    "# 資料載入和轉換\n",
    "encoded_data = []\n",
    "\n",
    "chars_to_remove = \"][' \"    \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    mbti_counts = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0,13:0,14:0,15:0}\n",
    "    mbti_per_count = []\n",
    "    dialogues = row[\"posts\"] #字串\n",
    "    target_personality = row[\"type\"]\n",
    "    for char in chars_to_remove:\n",
    "        dialogues = dialogues.replace(char, \"\")\n",
    "    \n",
    "    dialogues_list = dialogues.split(',')\n",
    "    \n",
    "    \n",
    "    dialogue_ids = [personality_mapping[personality] for personality in dialogues_list]\n",
    "    \n",
    "    for personality_id in dialogue_ids:\n",
    "        mbti_counts[personality_id] += 1\n",
    "    \n",
    "    for i in range(len(personality_mapping)):\n",
    "        mbti_per_count.append(round(mbti_counts[i]/len(dialogue_ids), 2))\n",
    "    \n",
    "    target_personality_id = personality_mapping[target_personality]\n",
    "    \n",
    "    encoded_data.append((mbti_per_count, target_personality_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e2c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列並轉換為張量\n",
    "input_data = torch.tensor([feature for feature, _ in encoded_data], dtype=torch.float32)\n",
    "target_personality = torch.tensor([target for _, target in encoded_data], dtype=torch.int64)  # 使用int64类型，因为它是类别标签\n",
    "\n",
    "# 資料集切分為訓練集和驗證集\n",
    "train_dialogues, val_dialogues, train_target, val_target = train_test_split(input_data, target_personality, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c98717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將你的訓練數據和測試數據轉換為NumPy數組\n",
    "X_train = train_dialogues.numpy()\n",
    "y_train = train_target.numpy()\n",
    "X_val = val_dialogues.numpy()\n",
    "Y_val = val_target.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e5a4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.14 0.21 0.   0.   0.21 0.14 0.14 0.14 0.   0.   0.   0.\n",
      " 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52c2900f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 轉換成1x16的圖像\n",
    "image_size = len(personality_mapping)\n",
    "\n",
    "# 创建一个空的列表来存储图像数据\n",
    "image_data = []\n",
    "\n",
    "for personality_num in X_train:\n",
    "    image_data.append(np.array(personality_num).reshape(1, 16))\n",
    "\n",
    "# 顯示圖像\n",
    "plt.imshow(image_data[0], cmap='viridis')  # 使用'viridis'色彩地圖，你可以根據需要更改色彩地圖\n",
    "\n",
    "image_data_val = []\n",
    "for personality_num in X_val:\n",
    "    image_data_val.append(np.array(personality_num).reshape(1, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f6a3725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.14 0.21 0.   0.   0.21 0.14 0.14 0.14 0.   0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JenMing\\AppData\\Local\\Temp\\ipykernel_22868\\3342659498.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  image_data_tensor = torch.tensor(image_data)\n"
     ]
    }
   ],
   "source": [
    "print(image_data[0])\n",
    "\n",
    "# 将列表 image_data 转换为 PyTorch 张量\n",
    "image_data_tensor = torch.tensor(image_data)\n",
    "\n",
    "image_data_val_tensor = torch.tensor(image_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b73ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器\n",
    "train_data = TensorDataset(image_data_tensor, train_target)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_data = TensorDataset(image_data_val_tensor, val_target)\n",
    "val_loader = DataLoader(val_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f6649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建CNN模型\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32*6*6, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32*6*6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd753462",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 64, 1, 16] to have 1 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[43], line 11\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 64, 1, 16] to have 1 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {(100 * val_correct / total):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eafbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估模型\n",
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {(100 * test_correct / total):.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
