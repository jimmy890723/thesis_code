{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6f8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8325e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>['INFP', 'INFP', 'INFJ', 'ENFP', 'ISTP', 'INTP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>['INTJ', 'INTP', 'ENFP', 'INTJ', 'INTP', 'INTP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>['INTJ', 'INFP', 'INFP', 'INTP', 'INTP', 'INTJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>['INTJ', 'ISFJ', 'INFP', 'INTP', 'INTP', 'INTP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>['ENTJ', 'INTP', 'ENFP', 'INTP', 'ENTJ', 'INTJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  ['INFP', 'INFP', 'INFJ', 'ENFP', 'ISTP', 'INTP...\n",
       "1  ENTP  ['INTJ', 'INTP', 'ENFP', 'INTJ', 'INTP', 'INTP...\n",
       "2  INTP  ['INTJ', 'INFP', 'INFP', 'INTP', 'INTP', 'INTJ...\n",
       "3  INTJ  ['INTJ', 'ISFJ', 'INFP', 'INTP', 'INTP', 'INTP...\n",
       "4  ENTJ  ['ENTJ', 'INTP', 'ENFP', 'INTP', 'ENTJ', 'INTJ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/JenMing/Desktop/MBTI/LSTM/mbti_to_LSTM_DF.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ffe4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編碼轉換\n",
    "personality_mapping = {'INFJ': 0,\n",
    "                        'ENTP': 1,\n",
    "                        'INTP': 2,\n",
    "                        'INTJ': 3,\n",
    "                        'ENTJ': 4,\n",
    "                        'ENFJ': 5,\n",
    "                        'INFP': 6,\n",
    "                        'ENFP': 7,\n",
    "                        'ISFP': 8,\n",
    "                        'ISTP': 9,\n",
    "                        'ISFJ': 10,\n",
    "                        'ISTJ': 11,\n",
    "                        'ESTP': 12,\n",
    "                        'ESFP': 13,\n",
    "                        'ESTJ': 14,\n",
    "                        'ESFJ': 15 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cdfddc",
   "metadata": {},
   "source": [
    "# 資料載入和轉換\n",
    "encoded_data = []\n",
    "\n",
    "chars_to_remove = \"][' \"    \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    dialogues = row[\"posts\"] #字串\n",
    "    target_personality = row[\"type\"]\n",
    "    for char in chars_to_remove:\n",
    "        dialogues = dialogues.replace(char, \"\")\n",
    "    \n",
    "    dialogues_list = dialogues.split(',')\n",
    "    \n",
    "    \n",
    "    dialogue_ids = [personality_mapping[personality] for personality in dialogues_list]\n",
    "    target_personality_id = personality_mapping[target_personality]\n",
    "    \n",
    "    encoded_data.append((dialogue_ids, target_personality_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52303fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料載入和轉換\n",
    "encoded_data = []\n",
    "\n",
    "chars_to_remove = \"][' \"    \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    mbti_counts = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0,13:0,14:0,15:0}\n",
    "    mbti_per_count = []\n",
    "    dialogues = row[\"posts\"] #字串\n",
    "    target_personality = row[\"type\"]\n",
    "    for char in chars_to_remove:\n",
    "        dialogues = dialogues.replace(char, \"\")\n",
    "    \n",
    "    dialogues_list = dialogues.split(',')\n",
    "    \n",
    "    \n",
    "    dialogue_ids = [personality_mapping[personality] for personality in dialogues_list]\n",
    "    \n",
    "    for personality_id in dialogue_ids:\n",
    "        mbti_counts[personality_id] += 1\n",
    "    \n",
    "    for i in range(len(personality_mapping)):\n",
    "        mbti_per_count.append(round(mbti_counts[i]/len(dialogue_ids), 2))\n",
    "    \n",
    "    target_personality_id = personality_mapping[target_personality]\n",
    "    \n",
    "    encoded_data.append((mbti_per_count, target_personality_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668f23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動態計算 input_size\n",
    "max_dialogue_length = max(len(dialogue) for dialogue, _ in encoded_data)\n",
    "input_size = max_dialogue_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b97d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充序列並轉換為張量\n",
    "padded_dialogues = [torch.tensor(dialogue, dtype=torch.float32) for dialogue, _ in encoded_data]\n",
    "padded_dialogues = pad_sequence(padded_dialogues, batch_first=True)\n",
    "\n",
    "target_personality = torch.tensor([target for _, target in encoded_data], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c45dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8674, 16])\n",
      "torch.Size([8674])\n"
     ]
    }
   ],
   "source": [
    "# 检查结果的形状\n",
    "print(padded_dialogues.shape)\n",
    "print(target_personality.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba920c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 ANN 模型\n",
    "class PersonalityPredictionANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob=0.5):\n",
    "        super(PersonalityPredictionANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7eddc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集切分為訓練集和驗證集\n",
    "train_dialogues, val_dialogues, train_target, val_target = train_test_split(padded_dialogues, target_personality, test_size=0.15, random_state=42)\n",
    "\n",
    "# 初始化模型\n",
    "hidden_size = 64\n",
    "output_size = len(personality_mapping)\n",
    "model = PersonalityPredictionANN(input_size, hidden_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439d498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義損失函數和優化器 (CEL:分類問題 MSE:回归问题)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.MSELoss() \n",
    "weight_decay = 0.01\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=weight_decay)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#StepLR 调度器会每隔 step_size 个周期将学习率乘以 gamma，以逐步降低学习率\n",
    "#scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2240f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#早停\n",
    "patience = 10  # 設定早期停止的耐心值\n",
    "best_val_loss = float('inf')\n",
    "counter = 0  # 用於計算連續的驗證損失沒有改善的次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01786e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Loss: 2.3076\n",
      "Validation Loss: 2.2102, Validation Accuracy: 23.2719%\n",
      "Epoch [2/60], Loss: 2.2639\n",
      "Validation Loss: 2.1914, Validation Accuracy: 23.2719%\n",
      "Epoch [3/60], Loss: 2.2426\n",
      "Validation Loss: 2.1727, Validation Accuracy: 23.5023%\n",
      "Epoch [4/60], Loss: 2.2039\n",
      "Validation Loss: 2.0812, Validation Accuracy: 31.0292%\n",
      "Epoch [5/60], Loss: 2.0991\n",
      "Validation Loss: 1.9977, Validation Accuracy: 33.1029%\n",
      "Epoch [6/60], Loss: 2.0601\n",
      "Validation Loss: 1.9801, Validation Accuracy: 33.3333%\n",
      "Epoch [7/60], Loss: 2.0439\n",
      "Validation Loss: 1.9654, Validation Accuracy: 33.3333%\n",
      "Epoch [8/60], Loss: 2.0371\n",
      "Validation Loss: 1.9662, Validation Accuracy: 33.4101%\n",
      "Epoch [9/60], Loss: 2.0333\n",
      "Validation Loss: 1.9690, Validation Accuracy: 33.9478%\n",
      "Epoch [10/60], Loss: 2.0281\n",
      "Validation Loss: 1.9614, Validation Accuracy: 33.7174%\n",
      "Epoch [11/60], Loss: 2.0296\n",
      "Validation Loss: 1.9558, Validation Accuracy: 34.3318%\n",
      "Epoch [12/60], Loss: 2.0284\n",
      "Validation Loss: 1.9596, Validation Accuracy: 34.1782%\n",
      "Epoch [13/60], Loss: 2.0242\n",
      "Validation Loss: 1.9556, Validation Accuracy: 34.4086%\n",
      "Epoch [14/60], Loss: 2.0240\n",
      "Validation Loss: 1.9590, Validation Accuracy: 34.5622%\n",
      "Epoch [15/60], Loss: 2.0261\n",
      "Validation Loss: 1.9562, Validation Accuracy: 34.5622%\n",
      "Epoch [16/60], Loss: 2.0259\n",
      "Validation Loss: 1.9571, Validation Accuracy: 34.4854%\n",
      "Epoch [17/60], Loss: 2.0231\n",
      "Validation Loss: 1.9570, Validation Accuracy: 34.5622%\n",
      "Epoch [18/60], Loss: 2.0228\n",
      "Validation Loss: 1.9584, Validation Accuracy: 34.6390%\n",
      "Epoch [19/60], Loss: 2.0222\n",
      "Validation Loss: 1.9622, Validation Accuracy: 34.3318%\n",
      "Epoch [20/60], Loss: 2.0223\n",
      "Validation Loss: 1.9569, Validation Accuracy: 34.1782%\n",
      "Epoch [21/60], Loss: 2.0219\n",
      "Validation Loss: 1.9586, Validation Accuracy: 34.4854%\n",
      "Epoch [22/60], Loss: 2.0159\n",
      "Validation Loss: 1.9564, Validation Accuracy: 34.9462%\n",
      "Epoch [23/60], Loss: 2.0193\n",
      "Validation Loss: 1.9556, Validation Accuracy: 34.3318%\n",
      "Epoch [24/60], Loss: 2.0280\n",
      "Validation Loss: 1.9574, Validation Accuracy: 33.4869%\n",
      "Epoch [25/60], Loss: 2.0196\n",
      "Validation Loss: 1.9642, Validation Accuracy: 34.7158%\n",
      "Epoch [26/60], Loss: 2.0220\n",
      "Validation Loss: 1.9595, Validation Accuracy: 34.5622%\n",
      "Epoch [27/60], Loss: 2.0230\n",
      "Validation Loss: 1.9557, Validation Accuracy: 34.1014%\n",
      "Epoch [28/60], Loss: 2.0231\n",
      "Validation Loss: 1.9584, Validation Accuracy: 34.4086%\n",
      "Epoch [29/60], Loss: 2.0237\n",
      "Validation Loss: 1.9591, Validation Accuracy: 33.7942%\n",
      "Epoch [30/60], Loss: 2.0199\n",
      "Validation Loss: 1.9575, Validation Accuracy: 34.4086%\n",
      "Epoch [31/60], Loss: 2.0218\n",
      "Validation Loss: 1.9544, Validation Accuracy: 34.2550%\n",
      "Epoch [32/60], Loss: 2.0226\n",
      "Validation Loss: 1.9567, Validation Accuracy: 34.6390%\n",
      "Epoch [33/60], Loss: 2.0184\n",
      "Validation Loss: 1.9558, Validation Accuracy: 34.4854%\n",
      "Epoch [34/60], Loss: 2.0186\n",
      "Validation Loss: 1.9567, Validation Accuracy: 34.7158%\n",
      "Epoch [35/60], Loss: 2.0204\n",
      "Validation Loss: 1.9584, Validation Accuracy: 34.2550%\n",
      "Epoch [36/60], Loss: 2.0214\n",
      "Validation Loss: 1.9586, Validation Accuracy: 34.5622%\n",
      "Epoch [37/60], Loss: 2.0214\n",
      "Validation Loss: 1.9573, Validation Accuracy: 34.4854%\n",
      "Epoch [38/60], Loss: 2.0160\n",
      "Validation Loss: 1.9544, Validation Accuracy: 34.5622%\n",
      "Epoch [39/60], Loss: 2.0209\n",
      "Validation Loss: 1.9539, Validation Accuracy: 34.2550%\n",
      "Epoch [40/60], Loss: 2.0243\n",
      "Validation Loss: 1.9565, Validation Accuracy: 34.6390%\n",
      "Epoch [41/60], Loss: 2.0213\n",
      "Validation Loss: 1.9598, Validation Accuracy: 34.4086%\n",
      "Epoch [42/60], Loss: 2.0222\n",
      "Validation Loss: 1.9584, Validation Accuracy: 34.8694%\n",
      "Epoch [43/60], Loss: 2.0196\n",
      "Validation Loss: 1.9545, Validation Accuracy: 34.3318%\n",
      "Epoch [44/60], Loss: 2.0215\n",
      "Validation Loss: 1.9594, Validation Accuracy: 34.3318%\n",
      "Epoch [45/60], Loss: 2.0199\n",
      "Validation Loss: 1.9526, Validation Accuracy: 34.2550%\n",
      "Epoch [46/60], Loss: 2.0201\n",
      "Validation Loss: 1.9578, Validation Accuracy: 34.9462%\n",
      "Epoch [47/60], Loss: 2.0184\n",
      "Validation Loss: 1.9535, Validation Accuracy: 34.7926%\n",
      "Epoch [48/60], Loss: 2.0206\n",
      "Validation Loss: 1.9546, Validation Accuracy: 34.7926%\n",
      "Epoch [49/60], Loss: 2.0222\n",
      "Validation Loss: 1.9536, Validation Accuracy: 34.4086%\n",
      "Epoch [50/60], Loss: 2.0199\n",
      "Validation Loss: 1.9545, Validation Accuracy: 34.8694%\n",
      "Epoch [51/60], Loss: 2.0169\n",
      "Validation Loss: 1.9542, Validation Accuracy: 34.4854%\n",
      "Epoch [52/60], Loss: 2.0217\n",
      "Validation Loss: 1.9567, Validation Accuracy: 34.5622%\n",
      "Epoch [53/60], Loss: 2.0185\n",
      "Validation Loss: 1.9545, Validation Accuracy: 34.7158%\n",
      "Epoch [54/60], Loss: 2.0215\n",
      "Validation Loss: 1.9569, Validation Accuracy: 34.2550%\n",
      "Epoch [55/60], Loss: 2.0229\n",
      "Validation Loss: 1.9537, Validation Accuracy: 35.0230%\n",
      "Early Stopping: Validation loss has not improved for 10 epochs. Stopping training.\n",
      "E.I: 56445/71610 \n",
      "Accuracy: 0.788227901131127\n",
      "\n",
      "S.N: 62645/71610 \n",
      "Accuracy: 0.8748079877112135\n",
      "\n",
      "T.F: 54859/71610 \n",
      "Accuracy: 0.7660801564027371\n",
      "\n",
      "J.P: 43649/71610 \n",
      "Accuracy: 0.6095377740539031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/JenMing/Desktop/MBTI/ANN/Model/note.txt\", \"w\") as f:\n",
    "    num_epochs = 60\n",
    "    dimension_counts = {'E/I': 0,\n",
    "                        'S/N': 0,\n",
    "                        'T/F': 0,\n",
    "                        'J/P': 0}\n",
    "    item_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 將模型設置為訓練模式\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for dialogue_batch, target_batch in zip(train_dialogues, train_target):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(dialogue_batch)\n",
    "            loss = criterion(outputs, target_batch.long())  # 使用交叉熵損失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_dialogues)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "        f.write(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\\n\")\n",
    "        \n",
    "        # 驗證模型\n",
    "        model.eval()  # 將模型設置為評估模式\n",
    "        correct_predictions = 0\n",
    "        total_samples = len(val_dialogues)\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for dialogue_batch, target_batch in zip(val_dialogues, val_target):\n",
    "                outputs = model(dialogue_batch)\n",
    "                loss = criterion(outputs, target_batch.long())  # 使用交叉熵損失\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_class = torch.argmax(outputs).item()\n",
    "                true_class = target_batch.item()\n",
    "\n",
    "                for personality, value in personality_mapping.items():\n",
    "                    if value == predicted_class:\n",
    "                        mbti_labels_pre = personality\n",
    "                        break\n",
    "                for personality, value in personality_mapping.items():\n",
    "                    if value == int(true_class):\n",
    "                        mbti_labels_tru = personality \n",
    "                        break\n",
    "\n",
    "                for n in range(4):\n",
    "                    if n == 0:\n",
    "                        if mbti_labels_pre[n] == mbti_labels_tru[n]:\n",
    "                            dimension_counts['E/I'] += 1\n",
    "                    elif n == 1:\n",
    "                        if mbti_labels_pre[n] == mbti_labels_tru[n]:\n",
    "                            dimension_counts['S/N'] += 1\n",
    "                    elif n == 2:\n",
    "                        if mbti_labels_pre[n] == mbti_labels_tru[n]:\n",
    "                            dimension_counts['T/F'] += 1\n",
    "                    elif n == 3:\n",
    "                        if mbti_labels_pre[n] == mbti_labels_tru[n]:\n",
    "                            dimension_counts['J/P'] += 1\n",
    "\n",
    "                if predicted_class == true_class:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "        item_count += total_samples\n",
    "        average_val_loss = val_loss / len(val_dialogues)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f\"Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {accuracy*100:.4f}%\")\n",
    "        f.write(f\"Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {accuracy*100:.4f}%\\n\")\n",
    "        \n",
    "        # 檢查驗證損失是否改善\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        # 如果連續一定次數（耐心值）驗證損失沒有改善，則停止訓練\n",
    "        if counter >= patience:\n",
    "            print(f\"Early Stopping: Validation loss has not improved for {patience} epochs. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    EI_counts = dimension_counts['E/I']\n",
    "    SN_counts = dimension_counts['S/N']\n",
    "    TF_counts = dimension_counts['T/F']\n",
    "    JP_counts = dimension_counts['J/P']\n",
    "    print(f'E.I: {EI_counts}/{item_count} ')\n",
    "    print('Accuracy: '+ str(EI_counts/item_count)+'\\n')\n",
    "    print(f'S.N: {SN_counts}/{item_count} ')\n",
    "    print('Accuracy: '+ str(SN_counts/item_count)+'\\n')\n",
    "    print(f'T.F: {TF_counts}/{item_count} ')\n",
    "    print('Accuracy: '+ str(TF_counts/item_count)+'\\n')\n",
    "    print(f'J.P: {JP_counts}/{item_count} ')\n",
    "    print('Accuracy: '+ str(JP_counts/item_count)+'\\n')\n",
    "    \n",
    "    f.write(f'E.I: {EI_counts}/{item_count} ')\n",
    "    f.write('Accuracy: '+ str(EI_counts/item_count)+'\\n')\n",
    "    f.write(f'S.N: {SN_counts}/{item_count} ')\n",
    "    f.write('Accuracy: '+ str(SN_counts/item_count)+'\\n')\n",
    "    f.write(f'T.F: {TF_counts}/{item_count} ')\n",
    "    f.write('Accuracy: '+ str(TF_counts/item_count)+'\\n')\n",
    "    f.write(f'J.P: {JP_counts}/{item_count} ')\n",
    "    f.write('Accuracy: '+ str(JP_counts/item_count)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b282ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"C:/Users/JenMing/Desktop/MBTI/ANN/Model/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6365f638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalityPredictionANN(\n",
       "  (fc1): Linear(in_features=16, out_features=64, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('C:/Users/JenMing/Desktop/MBTI/ANN/Model/lr=0.0005 3HL hd_size=64 l2=0.01/best_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08c9e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for dialogue_batch, target_batch in zip(val_dialogues, val_target):\n",
    "        outputs = model(dialogue_batch)\n",
    "\n",
    "        predicted_class = torch.argmax(outputs).item()\n",
    "        true_class = target_batch.item()\n",
    "        predictions.append(predicted_class)\n",
    "        labels.append(true_class)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a96f34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35023041474654376\n",
      "Precision: 0.2341618589859333\n",
      "Recall: 0.35023041474654376\n",
      "F1 Score: 0.24756675556946836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JenMing\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# 假設 predictions 和 labels 是你的預測和實際標籤\n",
    "#cm = confusion_matrix(labels, predictions)\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "precision = precision_score(labels, predictions, average='weighted')  # 可以使用 'micro'、'macro' 或 'weighted'\n",
    "recall = recall_score(labels, predictions, average='weighted')\n",
    "f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f3643",
   "metadata": {},
   "source": [
    "改了 正則化的參數 0.001 -> 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
